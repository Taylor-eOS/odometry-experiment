# odometry-experiment

The script begins by creating a small synthetic “world” made of 5 by 5 grayscale images. Each pixel grid is generated deterministically from its x and y coordinates so that every position (0,0) through (4,4) has its own unique 8×8 pattern. Those images are saved into a folder called grid_images. Next it reads each of those files back in, flattens the 8×8 pixel values into a 64-dimensional vector, and normalizes that vector so its length in Euclidean space is one. Those become the “embeddings” or fingerprints for each grid cell.

With those fingerprints in hand, the script assembles a training set by sampling random positions in the grid and random actions — north, south, east or west. For each sampled pair it records the source embedding, concatenates a one-hot representation of the chosen action, and pairs that with the embedding at the resulting cell after moving (or staying in place if the move would hit the grid boundary). Over two thousand such examples it trains a small two-layer feed-forward network that learns to predict the next embedding from the current embedding plus the action vector. The loss measures the mean squared error after re-normalizing the network’s output to unit length, which encourages the model to land its prediction back onto the normalized embedding manifold.

Finally the script picks a start cell (2,2) and a fixed sequence of moves “E, E, S, S, W.” For each move it feeds the current embedding and action into the trained network, normalizes its output, and then prints the grid cell it would occupy under that move alongside the cosine similarity between the predicted embedding and the true embedding of the new cell. The printed similarities show how closely the model’s imagined next view matches the actual fingerprint, effectively demonstrating that the network has learned the latent transition dynamics in this tiny synthetic world.
